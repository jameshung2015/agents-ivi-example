# Production patterns for LangChain agents with Streamlit

LangChain 0.3+ introduces fundamental architectural changes favoring **LangGraph** for multi-agent systems, **non-blocking callbacks by default**, and **structured output routing**. Combined with Streamlit 1.37+, these patterns enable building sophisticated, production-ready agent UIs with real-time visualization and workflow control.

## Agent routing fundamentally changed in LangChain 0.3+

LangChain 0.3+ deprecated legacy routing approaches like `initialize_agent()` and `AgentExecutor` chains in favor of **LangGraph-based orchestration**. The shift moves from sequential chain patterns to graph-based workflows with explicit state management and durable execution through automatic checkpointing.

Three core multi-agent patterns now dominate: **tool calling** (centralized controller), **handoffs** (decentralized agent-to-agent), and **supervisor** (orchestrator-worker). Tool calling works best when a single controller agent needs full context while delegating specific tasks. Handoffs enable direct agent-to-agent communication for complex specialist interactions. The supervisor pattern coordinates multiple workers with independent scratchpads, ideal for parallel processing and hierarchical team structures.

### Modern routing implementations leverage structured outputs

**LLM-based routing** now uses `.with_structured_output()` for reliable intent classification:

```python
from typing import Literal
from pydantic import BaseModel
from langchain_openai import ChatOpenAI

class RouteQuery(BaseModel):
    datasource: Literal["python_docs", "js_docs", "golang_docs"]
    confidence: float

llm = ChatOpenAI(model="gpt-4", temperature=0)
structured_llm = llm.with_structured_output(RouteQuery)

system = "Route user questions to the appropriate data source based on programming language."
router = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", "{question}")
]) | structured_llm

route = router.invoke({"question": "How do I use async in Python?"})
# Returns: RouteQuery(datasource="python_docs", confidence=0.95)
```

This approach handles complex, nuanced queries with natural language understanding but introduces LLM latency and costs. Choose LLM-based routing when query interpretation requires semantic understanding or when categories aren't clearly defined.

**Embedding-based routing** provides faster, deterministic alternatives for known intent categories:

```python
from langchain_openai import OpenAIEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

embeddings = OpenAIEmbeddings()
prompt_templates = [physics_template, math_template, chemistry_template]

# Pre-compute once
prompt_embeddings = embeddings.embed_documents(prompt_templates)

def route_query(query: str):
    query_embedding = embeddings.embed_query(query)
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]
    return prompt_templates[similarity.argmax()]
```

Embedding routing excels when you have 3-10 clear categories with distinct semantic boundaries, requiring sub-second response times. The **semantic-router** library optimizes this pattern for production with multi-modal support and dynamic routes.

### LangGraph workflows enable sophisticated routing logic

The modern pattern combines intent classification with conditional graph edges:

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, Literal

class AgentState(TypedDict):
    messages: list
    next: str

def classify_intent(state):
    router = llm.with_structured_output(Route)
    decision = router.invoke([
        {"role": "system", "content": "Classify request type"},
        {"role": "user", "content": state["messages"][-1].content}
    ])
    return {"next": decision.step}

workflow = StateGraph(AgentState)
workflow.add_node("classifier", classify_intent)
workflow.add_node("research_agent", research_chain)
workflow.add_node("code_agent", code_chain)
workflow.add_node("analysis_agent", analysis_chain)

workflow.add_edge(START, "classifier")
workflow.add_conditional_edges(
    "classifier",
    lambda x: x["next"],
    {"research": "research_agent", "code": "code_agent", "analysis": "analysis_agent"}
)

app = workflow.compile()
```

This architecture separates routing logic from execution, enables checkpointing at each node, supports human-in-the-loop interruption, and facilitates time-travel debugging. For production systems handling 10+ agent types or complex multi-step workflows, LangGraph provides the necessary orchestration infrastructure that sequential chains cannot match.

## Callbacks became non-blocking and require explicit handling

**The most critical LangChain 0.3+ breaking change**: callbacks now run asynchronously by default. In serverless environments (AWS Lambda, Cloudflare Workers, Vercel Functions), callbacks may not complete before function termination unless explicitly awaited.

```python
# Required for serverless environments
import os
os.environ["LANGCHAIN_CALLBACKS_BACKGROUND"] = "false"

# OR explicitly await callbacks
from langchain_core.callbacks.promises import awaitAllCallbacks
result = await agent_executor.ainvoke({"input": query})
await awaitAllCallbacks()
```

This change improves throughput for long-running agents but breaks existing serverless deployments that assumed blocking behavior. Always set `LANGCHAIN_CALLBACKS_BACKGROUND="false"` for AWS Lambda, Google Cloud Functions, or any environment with request timeout constraints.

### Custom callback handlers enable comprehensive monitoring

Implement `BaseCallbackHandler` for production-grade observability:

```python
from langchain_core.callbacks import BaseCallbackHandler
from uuid import UUID
import time

class ProductionCallback(BaseCallbackHandler):
    def __init__(self, logger, metrics):
        self.logger = logger
        self.metrics = metrics
        self.start_times = {}
        self.tool_counts = {}
    
    def on_agent_action(self, action, *, run_id: UUID, **kwargs):
        self.start_times[run_id] = time.time()
        self.tool_counts[action.tool] = self.tool_counts.get(action.tool, 0) + 1
        
        self.logger.info(
            "agent_action",
            extra={"tool": action.tool, "input": action.tool_input, "run_id": str(run_id)}
        )
    
    def on_tool_end(self, output: str, *, run_id: UUID, **kwargs):
        duration = time.time() - self.start_times.get(run_id, 0)
        self.metrics.histogram("agent.tool.duration", duration)
        self.metrics.increment("agent.tool.completed")
    
    def on_tool_error(self, error: Exception, *, run_id: UUID, **kwargs):
        self.metrics.increment("agent.tool.errors")
        self.logger.error(f"tool_error: {error}", exc_info=True)
```

The callback system provides **15+ event hooks** across chains, agents, tools, LLMs, and retrievers. Key events include `on_agent_action` (before tool execution), `on_tool_start/end/error` (tool lifecycle), `on_llm_new_token` (streaming tokens), and `on_custom_event` (user-defined progress tracking added in 0.2.14+).

**Three callback scoping levels** control when callbacks fire:

```python
# Constructor-level: All runs
agent_executor = AgentExecutor(agent=agent, tools=tools, callbacks=[callback1])

# Runtime-level: Single run  
result = agent_executor.invoke({"input": query}, {"callbacks": [callback2]})

# Chain-level: Reusable configuration
chain = agent_executor.with_config(callbacks=[callback3])
```

Constructor callbacks persist across all invocations, runtime callbacks apply only to that specific call, and chain-level callbacks create reusable configured instances. Combine multiple callbacks for different purposesâ€”one for metrics, one for logging, one for UI updates.

### Streaming architecture provides real-time visibility

The **`astream_events` API with version="v2"** is now the standard streaming interface (v1 deprecated, removed in 0.4):

```python
async for event in agent_executor.astream_events(
    {"input": "What's the weather in Tokyo?"},
    version="v2"
):
    kind = event["event"]
    
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        if content:
            print(content, end="", flush=True)
    
    elif kind == "on_tool_start":
        print(f"\nðŸ”§ Tool: {event['name']}")
        print(f"ðŸ“¥ Input: {event['data'].get('input')}")
    
    elif kind == "on_tool_end":
        print(f"ðŸ“¤ Output: {event['data'].get('output')}")
```

Version 2 simplified event structure, removed artifact events, improved chat model handling, and provided consistent output formats. Key differences from v1: direct access to `event["data"]["chunk"]` instead of nested `event["data"]["output"]["generations"]`, and removal of `on_retriever_stream` and `on_tool_stream` in favor of unified start/end events.

**Stream mode options** in LangGraph offer different granularity levels:

```python
# Mode 1: Updates (agent state changes)
for chunk in agent.stream(input, stream_mode="updates"):
    print(f"Node update: {chunk}")

# Mode 2: Messages (LLM tokens with metadata)
for token, metadata in agent.stream(input, stream_mode="messages"):
    print(f"Token from {metadata['langgraph_node']}: {token.content}")

# Mode 3: Custom (user-defined events)
from langgraph.config import get_stream_writer

@tool
def search(query: str):
    writer = get_stream_writer()
    writer(f"Searching: {query}")
    results = perform_search(query)
    writer(f"Found {len(results)} results")
    return results

# Mode 4: Multiple combined
for mode, chunk in agent.stream(input, stream_mode=["updates", "custom", "messages"]):
    handle_by_mode(mode, chunk)
```

Custom events enable progress tracking within long-running tools. Updates mode shows high-level agent state transitions. Messages mode provides token-level streaming with node attribution. Combine modes for comprehensive real-time visibility.

### Event filtering optimizes streaming performance

```python
async for event in agent_executor.astream_events(
    {"input": query},
    version="v2",
    include_names=["Agent", "SearchTool"],      # Only these components
    include_types=["chat_model", "tool"],        # Only these event types
    include_tags=["production"],                 # Only production-tagged
    exclude_names=["internal_debug_tool"]        # Skip internal tools
):
    process_event(event)
```

For high-throughput production systems, filtering reduces network overhead and processing load by 60-80% by excluding unnecessary events. Filter by component name, event type, or custom tags to stream only what the UI actually displays.

## Streamlit integration patterns enable real-time agent visualization

The official **`StreamlitCallbackHandler`** from `langchain_community` provides built-in agent visualization:

```python
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
import streamlit as st

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    
    with st.chat_message("assistant"):
        st_callback = StreamlitCallbackHandler(
            st.container(),
            max_thought_containers=4,
            expand_new_thoughts=True,
            collapse_completed_thoughts=True
        )
        
        response = agent_executor.invoke(
            {"input": prompt},
            {"callbacks": [st_callback]}
        )
        st.write(response["output"])
```

This handler automatically creates expandable sections for agent thoughts, displays tool execution with inputs/outputs, shows reasoning steps in real-time, and collapses completed thoughts to reduce visual clutter. The `max_thought_containers` parameter controls how many completed thought sections remain visibleâ€”set to 2-4 for optimal UX balancing detail with readability.

### Custom streaming handlers provide fine-grained control

For streaming beyond agent workflows, implement token-by-token display:

```python
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.run_id_ignore_token = None
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        # Filter out intermediate chain prompts
        if prompts[0].startswith("Human"):
            self.run_id_ignore_token = kwargs.get("run_id")
    
    def on_llm_new_token(self, token: str, **kwargs):
        if kwargs.get("run_id") == self.run_id_ignore_token:
            return
        
        self.text += token
        self.container.markdown(self.text)
    
    def on_llm_error(self, error, **kwargs):
        self.container.error(f"Error: {str(error)}")

# Usage
chat_box = st.empty()
handler = StreamHandler(chat_box)
chat = ChatOpenAI(model="gpt-4", streaming=True, callbacks=[handler])
response = chat.invoke([{"role": "user", "content": query}])
```

The `run_id_ignore_token` pattern prevents displaying intermediate chain outputs that users shouldn't see, like rephrased questions or internal tool queries. This maintains clean UIs showing only user-relevant content.

### Session state management maintains conversation context

Streamlit's `StreamlitChatMessageHistory` integrates directly with LangChain's memory system:

```python
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory

# Session-based history (persists during session)
msgs = StreamlitChatMessageHistory(key="langchain_messages")

if len(msgs.messages) == 0:
    msgs.add_ai_message("How can I help you?")

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

chain = prompt | ChatOpenAI()
chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: msgs,
    input_messages_key="question",
    history_messages_key="history"
)

# Display chat history
for msg in msgs.messages:
    st.chat_message(msg.type).write(msg.content)

# Handle new input
if prompt := st.chat_input():
    st.chat_message("human").write(prompt)
    
    config = {"configurable": {"session_id": "any"}}
    response = chain_with_history.invoke({"question": prompt}, config)
    
    st.chat_message("ai").write(response.content)
```

This pattern automatically handles message history persistence, maintains conversation context across reruns, and integrates with Streamlit's session state system. The `key` parameter namespaces history for multi-app deployments on the same Streamlit instance.

For **LangGraph workflows with checkpointing**:

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
import streamlit as st

@st.cache_resource
def get_checkpointer():
    return MemorySaver()

# Maintain thread_id in session state
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid4())

checkpointer = get_checkpointer()
agent = create_react_agent(llm, tools, checkpointer=checkpointer)

config = {"configurable": {"thread_id": st.session_state.thread_id}}
response = agent.invoke({"messages": [user_input]}, config)
```

The `@st.cache_resource` decorator ensures the checkpointer persists across reruns while `thread_id` maintains conversation continuity. LangGraph's checkpointing enables pause/resume workflows, time-travel debugging, and human-in-the-loop patterns.

### Multi-container layouts organize complex agent workflows

Separate containers for different information types improve UX:

```python
class MultiContainerCallback(BaseCallbackHandler):
    def __init__(self):
        self.thought_container = st.expander("ðŸ’­ Agent Reasoning", expanded=True)
        self.tool_container = st.expander("ðŸ”§ Tool Execution", expanded=False)
        self.metrics_container = st.sidebar.container()
        
        self.token_count = 0
        self.tool_usage = {}
    
    def on_llm_new_token(self, token: str, **kwargs):
        self.token_count += 1
        # Stream to main thought container
        with self.thought_container:
            st.markdown(self.current_thought + token)
    
    def on_tool_start(self, serialized, input_str, **kwargs):
        tool_name = serialized.get("name")
        self.tool_usage[tool_name] = self.tool_usage.get(tool_name, 0) + 1
        
        with self.tool_container:
            st.write(f"**{tool_name}**")
            st.code(input_str, language="text")
    
    def on_tool_end(self, output, **kwargs):
        with self.tool_container:
            st.success("âœ… Complete")
        
        # Update metrics sidebar
        with self.metrics_container:
            st.metric("Tokens", self.token_count)
            st.metric("Tools Used", sum(self.tool_usage.values()))
```

This pattern isolates agent reasoning (main expander), tool details (secondary expander), and execution metrics (sidebar). Users can focus on final output while expanding sections for debugging or learning about agent behavior.

### Progress tracking provides execution visibility

```python
class ProgressCallback(BaseCallbackHandler):
    def __init__(self, progress_bar, status_text):
        self.progress_bar = progress_bar
        self.status_text = status_text
        self.current_step = 0
        self.total_steps = 5  # Estimate
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        self.status_text.write("ðŸ”„ Initializing...")
        self.progress_bar.progress(0.1)
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        self.current_step += 1
        progress = min(0.2 + (self.current_step * 0.15), 0.8)
        self.status_text.write("ðŸ¤– Thinking...")
        self.progress_bar.progress(progress)
    
    def on_tool_start(self, serialized, input_str, **kwargs):
        tool_name = serialized.get("name")
        self.status_text.write(f"ðŸ”§ Using {tool_name}...")
        self.progress_bar.progress(0.6)
    
    def on_chain_end(self, outputs, **kwargs):
        self.status_text.write("âœ… Complete!")
        self.progress_bar.progress(1.0)

# Usage
progress_bar = st.progress(0)
status = st.empty()
callback = ProgressCallback(progress_bar, status)

response = agent_executor.invoke(
    {"input": query},
    {"callbacks": [callback]}
)
```

Progress bars reduce perceived latency for long-running operations. For agents with unpredictable tool chains, show relative progress (20% â†’ 40% â†’ 60%) rather than absolute percentages. Always reach 100% on completion to signal finality.

## Production deployment requires comprehensive error handling

Implement retry logic with exponential backoff for transient failures:

```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import streamlit as st

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((TimeoutError, ConnectionError))
)
async def execute_with_retry(agent_executor, query):
    try:
        return await agent_executor.ainvoke(
            {"input": query},
            {"callbacks": [ProductionCallback(logger, metrics)]}
        )
    except Exception as e:
        logger.error(f"Attempt failed: {e}", exc_info=True)
        raise

# Usage with error display
try:
    with st.spinner("Processing..."):
        response = await execute_with_retry(agent_executor, user_query)
        st.success("âœ… Complete")
except Exception as e:
    st.error(f"Request failed after 3 attempts: {str(e)}")
    if st.button("Retry"):
        st.rerun()
```

Configure retries only for transient errors (network issues, rate limits) not for errors requiring user intervention (invalid input, authentication failures). The exponential backoff prevents overwhelming downstream services during incidents.

### Resource caching optimizes performance

```python
@st.cache_resource
def get_agent_executor(api_key: str, model: str):
    """Cache agent to avoid recreation on each rerun"""
    llm = ChatOpenAI(api_key=api_key, model=model, streaming=True)
    tools = load_tools(["ddg-search", "llm-math"])
    agent = create_react_agent(llm, tools, prompt)
    
    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        max_iterations=10,
        max_execution_time=60,
        handle_parsing_errors=True
    )

@st.cache_resource
def get_embeddings():
    """Cache embedding model (expensive initialization)"""
    return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

@st.cache_resource(ttl="1h")
def get_vectorstore(_documents):
    """Cache with 1-hour expiration for dynamic data"""
    return FAISS.from_documents(_documents, get_embeddings())
```

Use `@st.cache_resource` for models, agents, and embedding instances that should persist across all users. Add `ttl` parameter for data that updates periodically. The underscore prefix (`_documents`) prevents Streamlit from hashing large objects, improving cache performance.

**Token tracking and cost monitoring** prevents budget overruns:

```python
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    response = chain.run(query)
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total Tokens", f"{cb.total_tokens:,}")
    col2.metric("Prompt Tokens", f"{cb.prompt_tokens:,}")
    col3.metric("Completion Tokens", f"{cb.completion_tokens:,}")
    col4.metric("Total Cost", f"${cb.total_cost:.4f}")
```

Display cost metrics in the sidebar or an expander to maintain awareness without cluttering the main interface. Set budget thresholds and halt execution when exceeded to prevent runaway costs during testing.

### Security patterns protect production deployments

```python
import streamlit as st
from pathlib import Path

# Use Streamlit secrets for API keys
if "openai" in st.secrets:
    api_key = st.secrets["openai"]["api_key"]
else:
    api_key = st.text_input("OpenAI API Key", type="password")
    if not api_key:
        st.warning("Please configure API key in .streamlit/secrets.toml")
        st.stop()

# Rate limiting with session state
if "request_count" not in st.session_state:
    st.session_state.request_count = 0
    st.session_state.request_reset_time = time.time()

# Reset counter every hour
if time.time() - st.session_state.request_reset_time > 3600:
    st.session_state.request_count = 0
    st.session_state.request_reset_time = time.time()

if st.session_state.request_count >= 50:
    st.error("Rate limit exceeded. Please try again later.")
    st.stop()

st.session_state.request_count += 1
```

Never commit API keys to repositories. Use `secrets.toml` for local development and environment variables for cloud deployments. Implement per-session rate limits to prevent abuse in public-facing applications.

## Migration checklist for existing deployments

**Update callback handling** for serverless environments:

```python
# Add to all AWS Lambda / Cloud Functions
import os
os.environ["LANGCHAIN_CALLBACKS_BACKGROUND"] = "false"

# OR use explicit await pattern
from langchain_core.callbacks.promises import awaitAllCallbacks
result = await agent.ainvoke(input)
await awaitAllCallbacks()
```

**Upgrade streaming API** from v1 to v2:

```python
# Before (v0.2 and earlier)
async for event in agent.astream_events(input, version="v1"):
    content = event["data"]["output"]["generations"][0]["message"]

# After (v0.3+)
async for event in agent.astream_events(input, version="v2"):
    content = event["data"]["chunk"]  # Simplified access
```

**Replace deprecated agent patterns**:

```python
# Before: AgentExecutor chains
from langchain.agents import initialize_agent
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

# After: LangGraph workflows  
from langgraph.prebuilt import create_react_agent
agent = create_react_agent(llm, tools, checkpointer=MemorySaver())
```

Legacy `initialize_agent()` will be removed in LangChain 0.4. Migrate to LangGraph for stateful workflows or use `create_react_agent()` for simple agents. The new patterns provide explicit state management, better error handling, and support for human-in-the-loop workflows.

## Key architectural decisions for production systems

**Choose routing strategy based on latency and flexibility requirements**: LLM-based routing provides maximum flexibility for complex queries but adds 200-500ms latency and costs $0.002-0.01 per route. Embedding-based routing delivers sub-50ms decisions deterministically but requires clear semantic boundaries between categories. For production systems handling 1000+ requests/hour with predictable patterns, semantic routing reduces costs by 10-20x while maintaining 95%+ accuracy.

**Structure multi-agent systems as graphs not chains**: LangGraph's explicit state management, checkpointing, and conditional routing enable complex workflows impossible with sequential chains. Use supervisor patterns for 5+ specialized agents, tool calling for hierarchical workflows, and handoffs when agents need direct user interaction. The investment in graph complexity pays off when agent count exceeds 3-4 or workflows require branching logic.

**Implement comprehensive observability from day one**: Production agent systems require callback handlers that track execution time, tool usage, error rates, token consumption, and user satisfaction. Integrate with existing observability platforms (DataDog, New Relic, Sentry) through custom callbacks. Log all routing decisions with confidence scores for continuous accuracy monitoring. Use LangSmith for detailed trace analysis during development and production debugging.

**Design UIs for transparency and control**: Users tolerate longer latency when they see progress. Display agent reasoning steps, tool executions, and intermediate results in real-time. Provide interactive controls to pause execution, modify parameters, or retry with different approaches. The StreamlitCallbackHandler automatically implements many best practices, but customize for domain-specific workflows to maximize user trust and engagement.

**Cache aggressively but invalidate intelligently**: Agent initialization, embedding models, and vector stores should persist using `@st.cache_resource`. Set appropriate TTLs for data that updatesâ€”1 hour for frequently changing content, 24 hours for static resources. Monitor cache hit rates and adjust granularity to balance freshness against performance. A well-tuned cache reduces API costs by 60-80% in production.

Production-ready LangChain agents with Streamlit UIs require understanding these architectural patterns, implementing comprehensive error handling, maintaining conversation state correctly, and providing real-time visibility into agent operations. The patterns presented here reflect battle-tested approaches from 2024-2025 deployments running LangChain 0.3+ and Streamlit 1.37+.